
				 Aum Sri Sai Ram
FER on AffectNet-8 using OADN 


Namespace(arch='resnet50', batch_size=64, batch_size_t=64, database='Affectnet', end2end=True, epochs=60, evaluate=False, imagesize=224, lr=0.001, model_dir='checkpoints_affectnet8', momentum=0.9, num_attentive_regions=25, num_classes=8, num_regions=4, pretrained='pretrainedmodels/resnet50-19c8e357.pth', print_freq=100, resume='', root_path='../data/AffectNetdataset/Manually_Annotated_Images_aligned/', start_epoch=0, train_landmarksfile='../data/Affectnetmetadata/training_affectnet_landmarks_scores.pkl', train_list='../data/Affectnetmetadata/training.csv', valid_landmarksfile='../data/Affectnetmetadata/validation_affectnet_landmarks_scores.pkl', valid_list='../data/Affectnetmetadata/validation.csv', weight_decay=1e-05, workers=16)

img_dir:  ../data/AffectNetdataset/Manually_Annotated_Images_aligned/

checkpoints:  checkpoints_affectnet8
Total included  287651 {0: 74874, 1: 134415, 2: 25459, 3: 14090, 4: 6378, 5: 3803, 6: 24882, 7: 3750}
Total included  4000 {0: 500, 1: 500, 2: 500, 3: 500, 4: 500, 5: 500, 6: 500, 7: 500}
length of  train Database for training: 287651
length of  test Database: 4000

Number of parameters:
Base Model: 23508032, Attention Branch:526600, Region Branch:526600 and Total: 24561232

Model loaded for layers 1-4 from vggface2 pretrainedmodels/vgg_msceleb_resnet50_ft_weight.pkl

Training starting:

Training Epoch: [0][0/4495]	att_loss  (2.076219081878662)	region_loss (9.529242515563965)	overall_loss (8.0386381149292)	Prec1  (9.375) 	
Training Epoch: [0][100/4495]	att_loss  (2.0795347666976474)	region_loss (7.502454370555311)	overall_loss (6.41787055931469)	Prec1  (32.858909606933594) 	
Training Epoch: [0][200/4495]	att_loss  (2.0797938019482056)	region_loss (7.0051668318943)	overall_loss (6.02009233076181)	Prec1  (38.83706283569336) 	
Training Epoch: [0][300/4495]	att_loss  (2.0795872718392814)	region_loss (6.707097145410075)	overall_loss (5.781595264954424)	Prec1  (41.87603759765625) 	
Training Epoch: [0][400/4495]	att_loss  (2.079574719926068)	region_loss (6.4774609610921425)	overall_loss (5.597883804777911)	Prec1  (44.08120346069336) 	
Training Epoch: [0][500/4495]	att_loss  (2.0795760035752773)	region_loss (6.321016994065153)	overall_loss (5.4727288939043905)	Prec1  (45.58071517944336) 	
Training Epoch: [0][600/4495]	att_loss  (2.0795308678796802)	region_loss (6.182134767935399)	overall_loss (5.361614080118062)	Prec1  (46.83340072631836) 	
Training Epoch: [0][700/4495]	att_loss  (2.0795544809349593)	region_loss (6.0666826366527955)	overall_loss (5.269257096863337)	Prec1  (47.76881408691406) 	
Training Epoch: [0][800/4495]	att_loss  (2.079597656795297)	region_loss (5.967400058824918)	overall_loss (5.189839669381188)	Prec1  (48.61891174316406) 	
Training Epoch: [0][900/4495]	att_loss  (2.079545707469775)	region_loss (5.878614207351379)	overall_loss (5.118800600678489)	Prec1  (49.39130401611328) 	
Training Epoch: [0][1000/4495]	att_loss  (2.0795432680493944)	region_loss (5.803783977186525)	overall_loss (5.058935924248024)	Prec1  (50.034339904785156) 	
Training Epoch: [0][1100/4495]	att_loss  (2.079518036664777)	region_loss (5.734931358741046)	overall_loss (5.003848780078524)	Prec1  (50.59321212768555) 	
Training Epoch: [0][1200/4495]	att_loss  (2.079535022564077)	region_loss (5.679409568851734)	overall_loss (4.959434741740421)	Prec1  (51.044700622558594) 	
Training Epoch: [0][1300/4495]	att_loss  (2.079536090714486)	region_loss (5.629288973027609)	overall_loss (4.919338477564994)	Prec1  (51.41838073730469) 	
Training Epoch: [0][1400/4495]	att_loss  (2.079516228057077)	region_loss (5.582637361251482)	overall_loss (4.882013215583022)	Prec1  (51.777748107910156) 	
Training Epoch: [0][1500/4495]	att_loss  (2.079459590645014)	region_loss (5.536671058564564)	overall_loss (4.845228846910237)	Prec1  (52.138160705566406) 	
Training Epoch: [0][1600/4495]	att_loss  (2.0794640573243064)	region_loss (5.4952055923943215)	overall_loss (4.812057367345082)	Prec1  (52.45940399169922) 	
Training Epoch: [0][1700/4495]	att_loss  (2.079468348108972)	region_loss (5.455237251810996)	overall_loss (4.780083551188205)	Prec1  (52.764915466308594) 	
Training Epoch: [0][1800/4495]	att_loss  (2.07946370999592)	region_loss (5.419306647704747)	overall_loss (4.751338139830001)	Prec1  (52.99746322631836) 	
Training Epoch: [0][1900/4495]	att_loss  (2.0794642527187452)	region_loss (5.384498331685997)	overall_loss (4.723491594077787)	Prec1  (53.284454345703125) 	
Training Epoch: [0][2000/4495]	att_loss  (2.079469947085745)	region_loss (5.351249432337397)	overall_loss (4.696893612662891)	Prec1  (53.48341751098633) 	
Training Epoch: [0][2100/4495]	att_loss  (2.079468625888888)	region_loss (5.31793017366964)	overall_loss (4.670237941551299)	Prec1  (53.76011657714844) 	
Training Epoch: [0][2200/4495]	att_loss  (2.079474874758168)	region_loss (5.28773203431233)	overall_loss (4.646080678834096)	Prec1  (54.01805877685547) 	
Training Epoch: [0][2300/4495]	att_loss  (2.0794690961891646)	region_loss (5.257037312193883)	overall_loss (4.621523743968899)	Prec1  (54.24271774291992) 	
Training Epoch: [0][2400/4495]	att_loss  (2.079477991376604)	region_loss (5.234061300233224)	overall_loss (4.603144711824518)	Prec1  (54.38554382324219) 	
Training Epoch: [0][2500/4495]	att_loss  (2.0794776481230324)	region_loss (5.206718163412125)	overall_loss (4.581270132861772)	Prec1  (54.564422607421875) 	
Training Epoch: [0][2600/4495]	att_loss  (2.079479842770792)	region_loss (5.181108078375453)	overall_loss (4.560782503595906)	Prec1  (54.73916244506836) 	
Training Epoch: [0][2700/4495]	att_loss  (2.0794794868425455)	region_loss (5.1534276742133684)	overall_loss (4.538638108132372)	Prec1  (54.9466667175293) 	
Training Epoch: [0][2800/4495]	att_loss  (2.079481725563368)	region_loss (5.128913348251392)	overall_loss (4.519027094635015)	Prec1  (55.10587692260742) 	
Training Epoch: [0][2900/4495]	att_loss  (2.079476017904298)	region_loss (5.1059003460616825)	overall_loss (4.500615550090181)	Prec1  (55.242801666259766) 	
Training Epoch: [0][3000/4495]	att_loss  (2.0794664172083883)	region_loss (5.085864704317031)	overall_loss (4.484585116442662)	Prec1  (55.39924240112305) 	
Training Epoch: [0][3100/4495]	att_loss  (2.0794510443108347)	region_loss (5.064523537979323)	overall_loss (4.4675091073344655)	Prec1  (55.56826400756836) 	
Training Epoch: [0][3200/4495]	att_loss  (2.0794504562492335)	region_loss (5.044866393186718)	overall_loss (4.451783272893084)	Prec1  (55.69890213012695) 	
Training Epoch: [0][3300/4495]	att_loss  (2.0794423228139336)	region_loss (5.025474443106318)	overall_loss (4.436268085929272)	Prec1  (55.86091613769531) 	
Training Epoch: [0][3400/4495]	att_loss  (2.079443966280324)	region_loss (5.005910884559382)	overall_loss (4.42061756723455)	Prec1  (56.007423400878906) 	
Training Epoch: [0][3500/4495]	att_loss  (2.079439665037916)	region_loss (4.9861061703099825)	overall_loss (4.404772935135641)	Prec1  (56.152706146240234) 	
Training Epoch: [0][3600/4495]	att_loss  (2.0794341013319655)	region_loss (4.9689293788294435)	overall_loss (4.39103038858553)	Prec1  (56.280372619628906) 	
Training Epoch: [0][3700/4495]	att_loss  (2.0794282396945785)	region_loss (4.951041680740299)	overall_loss (4.3767190571573416)	Prec1  (56.416343688964844) 	
Training Epoch: [0][3800/4495]	att_loss  (2.079420948179105)	region_loss (4.934555390796295)	overall_loss (4.363528566904929)	Prec1  (56.52665710449219) 	
Training Epoch: [0][3900/4495]	att_loss  (2.0794206918492986)	region_loss (4.9160141188864275)	overall_loss (4.348695497517706)	Prec1  (56.6829833984375) 	
Training Epoch: [0][4000/4495]	att_loss  (2.079422036518725)	region_loss (4.8975264111389905)	overall_loss (4.333905600035557)	Prec1  (56.819000244140625) 	
Training Epoch: [0][4100/4495]	att_loss  (2.0794169372013505)	region_loss (4.88060709192415)	overall_loss (4.3203691244997415)	Prec1  (56.962860107421875) 	
Training Epoch: [0][4200/4495]	att_loss  (2.0794114904782797)	region_loss (4.865185262929312)	overall_loss (4.308030571888754)	Prec1  (57.07606506347656) 	
Training Epoch: [0][4300/4495]	att_loss  (2.0794072903413823)	region_loss (4.849212384063182)	overall_loss (4.295251428679072)	Prec1  (57.190189361572266) 	
Training Epoch: [0][4400/4495]	att_loss  (2.0793992882017602)	region_loss (4.834706954700355)	overall_loss (4.283645484426351)	Prec1  (57.29450607299805) 	
Testing [62/63]	att_loss  (2.0794013748168947)	region_loss (4.663332317352295)	overall_loss (4.1465461769104)	Prec@1  (58.750003814697266)	
Epoch: 0   Test Acc: 58.750003814697266
Epoch: 0  Best so far Test Acc: 58.750003814697266
Training Epoch: [1][0/4495]	att_loss  (2.0793232917785645)	region_loss (4.330661773681641)	overall_loss (3.880394220352173)	Prec1  (59.375) 	
Training Epoch: [1][100/4495]	att_loss  (2.079045012445733)	region_loss (4.50630530980554)	overall_loss (4.020853283381698)	Prec1  (58.694305419921875) 	
Training Epoch: [1][200/4495]	att_loss  (2.079229562436763)	region_loss (4.507182895840697)	overall_loss (4.021592288468015)	Prec1  (58.97854232788086) 	
Training Epoch: [1][300/4495]	att_loss  (2.079301340635433)	region_loss (4.483918974169861)	overall_loss (4.00299551083004)	Prec1  (59.473628997802734) 	
Training Epoch: [1][400/4495]	att_loss  (2.0793544716965826)	region_loss (4.464785894550885)	overall_loss (3.9876996638471645)	Prec1  (59.64385986328125) 	
Training Epoch: [1][500/4495]	att_loss  (2.0793297100447847)	region_loss (4.44574130509428)	overall_loss (3.9724590430954496)	Prec1  (59.76796340942383) 	
Training Epoch: [1][600/4495]	att_loss  (2.0792727890903264)	region_loss (4.437305261608765)	overall_loss (3.965698824944393)	Prec1  (59.90016555786133) 	
Training Epoch: [1][700/4495]	att_loss  (2.079282673891533)	region_loss (4.41208234901265)	overall_loss (3.9455224675220704)	Prec1  (59.99242401123047) 	
Training Epoch: [1][800/4495]	att_loss  (2.079278511054507)	region_loss (4.388055515051185)	overall_loss (3.926300166697984)	Prec1  (60.24695587158203) 	
Training Epoch: [1][900/4495]	att_loss  (2.079271955839405)	region_loss (4.386570202788291)	overall_loss (3.9251106052631544)	Prec1  (60.24556350708008) 	
Training Epoch: [1][1000/4495]	att_loss  (2.079287917225749)	region_loss (4.375801195036043)	overall_loss (3.9164985898253204)	Prec1  (60.34902572631836) 	
Training Epoch: [1][1100/4495]	att_loss  (2.0792825029288284)	region_loss (4.3630545791120126)	overall_loss (3.9063002149372292)	Prec1  (60.49046325683594) 	
Training Epoch: [1][1200/4495]	att_loss  (2.079279612343476)	region_loss (4.354098567756189)	overall_loss (3.8991348290820604)	Prec1  (60.56411361694336) 	
Training Epoch: [1][1300/4495]	att_loss  (2.0792747648929653)	region_loss (4.349817959003317)	overall_loss (3.895709372226501)	Prec1  (60.59401321411133) 	
Training Epoch: [1][1400/4495]	att_loss  (2.0792692686131304)	region_loss (4.3375278325867095)	overall_loss (3.885876171934358)	Prec1  (60.71444320678711) 	
Training Epoch: [1][1500/4495]	att_loss  (2.079255447476646)	region_loss (4.326381165214096)	overall_loss (3.8769560727494943)	Prec1  (60.812583923339844) 	
Training Epoch: [1][1600/4495]	att_loss  (2.07925925963674)	region_loss (4.315425837211204)	overall_loss (3.8681925734008273)	Prec1  (60.9238395690918) 	
Training Epoch: [1][1700/4495]	att_loss  (2.0792538955728843)	region_loss (4.3020931306409524)	overall_loss (3.8575253359084267)	Prec1  (60.99904251098633) 	
Training Epoch: [1][1800/4495]	att_loss  (2.0792564265533398)	region_loss (4.293252073613092)	overall_loss (3.850452997762584)	Prec1  (61.111881256103516) 	
Training Epoch: [1][1900/4495]	att_loss  (2.0792525987258905)	region_loss (4.284341324636148)	overall_loss (3.843323633584269)	Prec1  (61.21367263793945) 	
Training Epoch: [1][2000/4495]	att_loss  (2.0792587274077654)	region_loss (4.278176535849926)	overall_loss (3.838393027993335)	Prec1  (61.24281692504883) 	
Training Epoch: [1][2100/4495]	att_loss  (2.079256796371591)	region_loss (4.268422821032212)	overall_loss (3.830589669820866)	Prec1  (61.34058380126953) 	
Training Epoch: [1][2200/4495]	att_loss  (2.079252773607281)	region_loss (4.2627214515604575)	overall_loss (3.8260277690046864)	Prec1  (61.433013916015625) 	
Training Epoch: [1][2300/4495]	att_loss  (2.079245589599046)	region_loss (4.251189405842482)	overall_loss (3.8168006950438103)	Prec1  (61.53710174560547) 	
Training Epoch: [1][2400/4495]	att_loss  (2.079245881307428)	region_loss (4.242659776322993)	overall_loss (3.8099770499288614)	Prec1  (61.58957290649414) 	
Training Epoch: [1][2500/4495]	att_loss  (2.0792494385493177)	region_loss (4.23659783141797)	overall_loss (3.805128204684313)	Prec1  (61.659088134765625) 	
Training Epoch: [1][2600/4495]	att_loss  (2.0792513781902837)	region_loss (4.229276520249845)	overall_loss (3.7992715431332176)	Prec1  (61.751487731933594) 	
Training Epoch: [1][2700/4495]	att_loss  (2.0792521032215445)	region_loss (4.220738513784822)	overall_loss (3.7924412827454685)	Prec1  (61.83705520629883) 	
Training Epoch: [1][2800/4495]	att_loss  (2.079250114809313)	region_loss (4.21155507945027)	overall_loss (3.7850941371509155)	Prec1  (61.904232025146484) 	
Training Epoch: [1][2900/4495]	att_loss  (2.079245797932785)	region_loss (4.2082572301393215)	overall_loss (3.7824549937979675)	Prec1  (61.91668701171875) 	
Training Epoch: [1][3000/4495]	att_loss  (2.079243390332139)	region_loss (4.200226723611533)	overall_loss (3.776030106927426)	Prec1  (61.97309494018555) 	
Training Epoch: [1][3100/4495]	att_loss  (2.079240195253748)	region_loss (4.194901595151029)	overall_loss (3.771769364854283)	Prec1  (62.00822067260742) 	
Training Epoch: [1][3200/4495]	att_loss  (2.0792393998703482)	region_loss (4.188346435337728)	overall_loss (3.7665250777304453)	Prec1  (62.06703186035156) 	
Training Epoch: [1][3300/4495]	att_loss  (2.079236060696925)	region_loss (4.182376071634671)	overall_loss (3.761748118893156)	Prec1  (62.1284294128418) 	
Training Epoch: [1][3400/4495]	att_loss  (2.079236146107522)	region_loss (4.178680415084803)	overall_loss (3.758791610445215)	Prec1  (62.153133392333984) 	
Training Epoch: [1][3500/4495]	att_loss  (2.0792380724931165)	region_loss (4.1736946801259425)	overall_loss (3.754803407536136)	Prec1  (62.2041015625) 	
Training Epoch: [1][3600/4495]	att_loss  (2.079237741220332)	region_loss (4.171184464316672)	overall_loss (3.752795168506672)	Prec1  (62.201904296875) 	
Training Epoch: [1][3700/4495]	att_loss  (2.0792345433130808)	region_loss (4.16447759370358)	overall_loss (3.7474290320951207)	Prec1  (62.23276138305664) 	
Training Epoch: [1][3800/4495]	att_loss  (2.0792347820330157)	region_loss (4.159087553490967)	overall_loss (3.743117047811426)	Prec1  (62.28706741333008) 	
Training Epoch: [1][3900/4495]	att_loss  (2.0792333655588386)	region_loss (4.151982264224152)	overall_loss (3.7374325332137994)	Prec1  (62.34819793701172) 	
Training Epoch: [1][4000/4495]	att_loss  (2.0792324583520294)	region_loss (4.14521660592609)	overall_loss (3.7320198254178862)	Prec1  (62.42072677612305) 	
Training Epoch: [1][4100/4495]	att_loss  (2.0792353632275113)	region_loss (4.140105951474777)	overall_loss (3.7279318830671033)	Prec1  (62.47370910644531) 	
Training Epoch: [1][4200/4495]	att_loss  (2.0792345037008575)	region_loss (4.133760877122087)	overall_loss (3.7228556521533074)	Prec1  (62.533843994140625) 	
Training Epoch: [1][4300/4495]	att_loss  (2.07923475511405)	region_loss (4.127309924473571)	overall_loss (3.7176949399705546)	Prec1  (62.59445571899414) 	
Training Epoch: [1][4400/4495]	att_loss  (2.0792340424569513)	region_loss (4.121023503576347)	overall_loss (3.7126656610081072)	Prec1  (62.64449691772461) 	
Testing [62/63]	att_loss  (2.0792817249298094)	region_loss (4.7591214294433595)	overall_loss (4.223153560638428)	Prec@1  (58.920001525878906)	
Epoch: 1   Test Acc: 58.920001525878906
Epoch: 1  Best so far Test Acc: 58.920001525878906
Training Epoch: [2][0/4495]	att_loss  (2.080705404281616)	region_loss (3.321237325668335)	overall_loss (3.0731308460235596)	Prec1  (71.875) 	
Training Epoch: [2][100/4495]	att_loss  (2.079106422934202)	region_loss (3.7965229832299867)	overall_loss (3.4530397240478212)	Prec1  (65.84158325195312) 	
Training Epoch: [2][200/4495]	att_loss  (2.0791432371186973)	region_loss (3.831823096346499)	overall_loss (3.4812871686261686)	Prec1  (65.41510772705078) 	
Training Epoch: [2][300/4495]	att_loss  (2.07917168370117)	region_loss (3.8351356856450685)	overall_loss (3.483942930088487)	Prec1  (65.28758239746094) 	
Training Epoch: [2][400/4495]	att_loss  (2.0791682596515835)	region_loss (3.842401460519158)	overall_loss (3.489754864699823)	Prec1  (65.3717269897461) 	
Training Epoch: [2][500/4495]	att_loss  (2.0791631644357462)	region_loss (3.8452905371279535)	overall_loss (3.4920651070372073)	Prec1  (65.16342163085938) 	
Training Epoch: [2][600/4495]	att_loss  (2.0791806377309334)	region_loss (3.853460943441026)	overall_loss (3.4986049276025044)	Prec1  (65.11802673339844) 	
Training Epoch: [2][700/4495]	att_loss  (2.0791873618981636)	region_loss (3.842015146358887)	overall_loss (3.489449636402212)	Prec1  (65.21710205078125) 	
Training Epoch: [2][800/4495]	att_loss  (2.079189975312289)	region_loss (3.838044167160244)	overall_loss (3.4862733753432944)	Prec1  (65.23876190185547) 	
Training Epoch: [2][900/4495]	att_loss  (2.0792054415542993)	region_loss (3.8312434837900176)	overall_loss (3.4808359217564355)	Prec1  (65.31285095214844) 	
Training Epoch: [2][1000/4495]	att_loss  (2.079204950894747)	region_loss (3.8321820112851475)	overall_loss (3.4815866466049665)	Prec1  (65.30968475341797) 	
Training Epoch: [2][1100/4495]	att_loss  (2.079212401370153)	region_loss (3.8265348407596376)	overall_loss (3.4770703997858865)	Prec1  (65.35394287109375) 	
Training Epoch: [2][1200/4495]	att_loss  (2.079207687155591)	region_loss (3.8161567839655053)	overall_loss (3.4687670120887217)	Prec1  (65.38951873779297) 	
Training Epoch: [2][1300/4495]	att_loss  (2.0792082023107485)	region_loss (3.806050088202193)	overall_loss (3.4606817578646334)	Prec1  (65.45325469970703) 	
Training Epoch: [2][1400/4495]	att_loss  (2.079213814936222)	region_loss (3.7993783123743357)	overall_loss (3.455345459617435)	Prec1  (65.46997833251953) 	
Training Epoch: [2][1500/4495]	att_loss  (2.0792138781410943)	region_loss (3.7881125015548514)	overall_loss (3.4463328223955942)	Prec1  (65.56670379638672) 	
Training Epoch: [2][1600/4495]	att_loss  (2.079213085508138)	region_loss (3.781042299890131)	overall_loss (3.4406765024041026)	Prec1  (65.630859375) 	
Training Epoch: [2][1700/4495]	att_loss  (2.079207747631533)	region_loss (3.778268862443416)	overall_loss (3.4384566843334468)	Prec1  (65.63785552978516) 	
Training Epoch: [2][1800/4495]	att_loss  (2.0792078013422755)	region_loss (3.7683562787620972)	overall_loss (3.430526628287748)	Prec1  (65.76380920410156) 	
Training Epoch: [2][1900/4495]	att_loss  (2.079206340630264)	region_loss (3.76473320665515)	overall_loss (3.427627878023536)	Prec1  (65.78856658935547) 	
Training Epoch: [2][2000/4495]	att_loss  (2.079212506254693)	region_loss (3.7606370080893545)	overall_loss (3.4243521527133542)	Prec1  (65.78976440429688) 	
Training Epoch: [2][2100/4495]	att_loss  (2.079215743778888)	region_loss (3.75803559068156)	overall_loss (3.422271665875427)	Prec1  (65.82431030273438) 	
Training Epoch: [2][2200/4495]	att_loss  (2.0792154920258232)	region_loss (3.756373324539379)	overall_loss (3.4209418028173313)	Prec1  (65.84294128417969) 	
Training Epoch: [2][2300/4495]	att_loss  (2.0792174030521133)	region_loss (3.7493110271911423)	overall_loss (3.4152923474773953)	Prec1  (65.91970825195312) 	
Training Epoch: [2][2400/4495]	att_loss  (2.079212753537395)	region_loss (3.7451305665457464)	overall_loss (3.4119470494829183)	Prec1  (65.94712829589844) 	
Training Epoch: [2][2500/4495]	att_loss  (2.0792086432333803)	region_loss (3.7416067933712136)	overall_loss (3.4091272091970404)	Prec1  (65.99797821044922) 	
Training Epoch: [2][2600/4495]	att_loss  (2.079212099256446)	region_loss (3.7373276533231694)	overall_loss (3.405704588305258)	Prec1  (66.03829956054688) 	
Training Epoch: [2][2700/4495]	att_loss  (2.079216551048056)	region_loss (3.733654973860892)	overall_loss (3.4027673348988574)	Prec1  (66.07853698730469) 	
Training Epoch: [2][2800/4495]	att_loss  (2.0792190941773496)	region_loss (3.727407727995671)	overall_loss (3.397770047315484)	Prec1  (66.12203216552734) 	
Training Epoch: [2][2900/4495]	att_loss  (2.0792211750711664)	region_loss (3.723287544592378)	overall_loss (3.394474317024018)	Prec1  (66.1948471069336) 	
Training Epoch: [2][3000/4495]	att_loss  (2.079219071080946)	region_loss (3.7194943260407696)	overall_loss (3.391439321080036)	Prec1  (66.22532653808594) 	
Training Epoch: [2][3100/4495]	att_loss  (2.079220126575518)	region_loss (3.7149979854468107)	overall_loss (3.387842460049233)	Prec1  (66.28103637695312) 	
Training Epoch: [2][3200/4495]	att_loss  (2.079218850140272)	region_loss (3.7121606381823593)	overall_loss (3.385572326887179)	Prec1  (66.30935668945312) 	
Training Epoch: [2][3300/4495]	att_loss  (2.0792132674907418)	region_loss (3.7096466049574532)	overall_loss (3.383559983919982)	Prec1  (66.33454132080078) 	
Training Epoch: [2][3400/4495]	att_loss  (2.079214256043225)	region_loss (3.7060709724493566)	overall_loss (3.380699675772268)	Prec1  (66.39131927490234) 	
Training Epoch: [2][3500/4495]	att_loss  (2.079213298276233)	region_loss (3.701651130598363)	overall_loss (3.377163610877871)	Prec1  (66.43013000488281) 	
Training Epoch: [2][3600/4495]	att_loss  (2.0792129585988586)	region_loss (3.6947792705910896)	overall_loss (3.371666055095093)	Prec1  (66.49758911132812) 	
Training Epoch: [2][3700/4495]	att_loss  (2.0792162274064583)	region_loss (3.6889779593486525)	overall_loss (3.3670256597807007)	Prec1  (66.55760192871094) 	
Training Epoch: [2][3800/4495]	att_loss  (2.079218453841597)	region_loss (3.684301248947842)	overall_loss (3.363284737083417)	Prec1  (66.5902099609375) 	
Training Epoch: [2][3900/4495]	att_loss  (2.079219389438507)	region_loss (3.6827127363888983)	overall_loss (3.3620141141691504)	Prec1  (66.60832214355469) 	
Training Epoch: [2][4000/4495]	att_loss  (2.0792169917139285)	region_loss (3.679401005485361)	overall_loss (3.3593642504267085)	Prec1  (66.65482330322266) 	
Training Epoch: [2][4100/4495]	att_loss  (2.0792168295985167)	region_loss (3.6744727425039234)	overall_loss (3.3554216080600243)	Prec1  (66.7184829711914) 	
Training Epoch: [2][4200/4495]	att_loss  (2.07921310834333)	region_loss (3.669941608181059)	overall_loss (3.351795956623665)	Prec1  (66.74526977539062) 	
Training Epoch: [2][4300/4495]	att_loss  (2.079212344654547)	region_loss (3.6645303715996564)	overall_loss (3.347466814881063)	Prec1  (66.80496215820312) 	
Training Epoch: [2][4400/4495]	att_loss  (2.079213661054296)	region_loss (3.659758276030357)	overall_loss (3.3436494018781784)	Prec1  (66.8385009765625) 	
Testing [62/63]	att_loss  (2.079261392593384)	region_loss (5.097433254241944)	overall_loss (4.493798984527588)	Prec@1  (57.95000457763672)	
Epoch: 2   Test Acc: 57.95000457763672
Training Epoch: [3][0/4495]	att_loss  (2.0785253047943115)	region_loss (3.0898048877716064)	overall_loss (2.8875489234924316)	Prec1  (73.4375) 	
Training Epoch: [3][100/4495]	att_loss  (2.079138186898562)	region_loss (3.433961788026413)	overall_loss (3.1629971135960946)	Prec1  (68.50247192382812) 	
Training Epoch: [3][200/4495]	att_loss  (2.0791797827725387)	region_loss (3.483267488764293)	overall_loss (3.2024499943007285)	Prec1  (68.35354614257812) 	
Training Epoch: [3][300/4495]	att_loss  (2.079223295382883)	region_loss (3.436258794461374)	overall_loss (3.164851746289833)	Prec1  (68.7915267944336) 	
Training Epoch: [3][400/4495]	att_loss  (2.0792107736677896)	region_loss (3.4290560986335734)	overall_loss (3.1590870812052207)	Prec1  (69.02275848388672) 	
Training Epoch: [3][500/4495]	att_loss  (2.0792226729516736)	region_loss (3.430599086060971)	overall_loss (3.1603238539781398)	Prec1  (69.0556411743164) 	
Training Epoch: [3][600/4495]	att_loss  (2.079223591952078)	region_loss (3.4280482859857466)	overall_loss (3.158283398076024)	Prec1  (69.1061782836914) 	
Training Epoch: [3][700/4495]	att_loss  (2.0792289725724027)	region_loss (3.4245264103341206)	overall_loss (3.155466972336109)	Prec1  (69.26043701171875) 	
Training Epoch: [3][800/4495]	att_loss  (2.0792360005158463)	region_loss (3.4220650346389276)	overall_loss (3.153499276301685)	Prec1  (69.22596740722656) 	
Training Epoch: [3][900/4495]	att_loss  (2.0792380768503915)	region_loss (3.4143418346208154)	overall_loss (3.147321133317217)	Prec1  (69.29973602294922) 	
Training Epoch: [3][1000/4495]	att_loss  (2.079240155386758)	region_loss (3.4124507076375847)	overall_loss (3.145808647562574)	Prec1  (69.36188507080078) 	
Training Epoch: [3][1100/4495]	att_loss  (2.0792351977376913)	region_loss (3.4089936074292413)	overall_loss (3.1430419759247976)	Prec1  (69.46100616455078) 	
Training Epoch: [3][1200/4495]	att_loss  (2.079234938538144)	region_loss (3.4062445593515505)	overall_loss (3.1408426853937472)	Prec1  (69.49286651611328) 	
Training Epoch: [3][1300/4495]	att_loss  (2.0792402420293175)	region_loss (3.4054773367889837)	overall_loss (3.1402299681780064)	Prec1  (69.46339416503906) 	
Training Epoch: [3][1400/4495]	att_loss  (2.079236271890209)	region_loss (3.3987869475246923)	overall_loss (3.1348768617322325)	Prec1  (69.50057983398438) 	
Training Epoch: [3][1500/4495]	att_loss  (2.079228947116565)	region_loss (3.4020228486788584)	overall_loss (3.1374641177179337)	Prec1  (69.43912506103516) 	
Training Epoch: [3][1600/4495]	att_loss  (2.079234108487045)	region_loss (3.3972746306549824)	overall_loss (3.1336665757368087)	Prec1  (69.44683074951172) 	
Training Epoch: [3][1700/4495]	att_loss  (2.079233121199162)	region_loss (3.3988809883419306)	overall_loss (3.134951464573402)	Prec1  (69.42423248291016) 	
Training Epoch: [3][1800/4495]	att_loss  (2.079232835293081)	region_loss (3.3925508553289427)	overall_loss (3.129887300819109)	Prec1  (69.51519775390625) 	
Training Epoch: [3][1900/4495]	att_loss  (2.079228195374041)	region_loss (3.3912709708090896)	overall_loss (3.128862464572178)	Prec1  (69.53823852539062) 	
Training Epoch: [3][2000/4495]	att_loss  (2.079224159454239)	region_loss (3.385531649417963)	overall_loss (3.124270200431496)	Prec1  (69.60426330566406) 	
Training Epoch: [3][2100/4495]	att_loss  (2.079221679391548)	region_loss (3.3793211232361937)	overall_loss (3.11930128331981)	Prec1  (69.64094543457031) 	
Training Epoch: [3][2200/4495]	att_loss  (2.0792175269787663)	region_loss (3.376696536184603)	overall_loss (3.1172007824929397)	Prec1  (69.69772338867188) 	
Training Epoch: [3][2300/4495]	att_loss  (2.079217186289111)	region_loss (3.372361836926619)	overall_loss (3.1137329543066876)	Prec1  (69.74005889892578) 	
Training Epoch: [3][2400/4495]	att_loss  (2.0792138656940327)	region_loss (3.3663121035872576)	overall_loss (3.1088925036029584)	Prec1  (69.79318237304688) 	
Training Epoch: [3][2500/4495]	att_loss  (2.0792115238941653)	region_loss (3.3634402943057853)	overall_loss (3.106594587268471)	Prec1  (69.82894134521484) 	
Training Epoch: [3][2600/4495]	att_loss  (2.0792120582825495)	region_loss (3.359327593614211)	overall_loss (3.1033045334891143)	Prec1  (69.85955047607422) 	
Training Epoch: [3][2700/4495]	att_loss  (2.0792117698772534)	region_loss (3.357310945967399)	overall_loss (3.101691157718095)	Prec1  (69.89193725585938) 	
Training Epoch: [3][2800/4495]	att_loss  (2.079211556200724)	region_loss (3.355296939536956)	overall_loss (3.1000799093021745)	Prec1  (69.90026092529297) 	
Training Epoch: [3][2900/4495]	att_loss  (2.079214662652474)	region_loss (3.3525952249508566)	overall_loss (3.0979191589174664)	Prec1  (69.9360122680664) 	
Training Epoch: [3][3000/4495]	att_loss  (2.079213989372851)	region_loss (3.348416220502272)	overall_loss (3.0945758201884495)	Prec1  (69.96834564208984) 	
Training Epoch: [3][3100/4495]	att_loss  (2.0792133515975197)	region_loss (3.344422977995542)	overall_loss (3.0913810981008094)	Prec1  (70.02227020263672) 	
Training Epoch: [3][3200/4495]	att_loss  (2.079214211517258)	region_loss (3.3402351689539787)	overall_loss (3.0880310229680124)	Prec1  (70.05818939208984) 	
Training Epoch: [3][3300/4495]	att_loss  (2.0792111392022625)	region_loss (3.3402276352585103)	overall_loss (3.0880243819036544)	Prec1  (70.0611572265625) 	
Training Epoch: [3][3400/4495]	att_loss  (2.0792110637158934)	region_loss (3.336706110955126)	overall_loss (3.085207146674316)	Prec1  (70.09381103515625) 	
Training Epoch: [3][3500/4495]	att_loss  (2.0792110289072725)	region_loss (3.333665112705443)	overall_loss (3.0827743408169894)	Prec1  (70.11166381835938) 	
Training Epoch: [3][3600/4495]	att_loss  (2.0792136770988363)	region_loss (3.3283319035625167)	overall_loss (3.0785083025569753)	Prec1  (70.15933227539062) 	
Training Epoch: [3][3700/4495]	att_loss  (2.079216236232004)	region_loss (3.3246438595386043)	overall_loss (3.0755583790629917)	Prec1  (70.19175720214844) 	
Training Epoch: [3][3800/4495]	att_loss  (2.079215101303787)	region_loss (3.320218614138167)	overall_loss (3.0720179559242973)	Prec1  (70.24015045166016) 	
Training Epoch: [3][3900/4495]	att_loss  (2.0792134958974704)	region_loss (3.315105069022825)	overall_loss (3.0679267985916723)	Prec1  (70.31970977783203) 	
Training Epoch: [3][4000/4495]	att_loss  (2.079212046777925)	region_loss (3.3105862962100185)	overall_loss (3.0643114903902178)	Prec1  (70.36288452148438) 	
Training Epoch: [3][4100/4495]	att_loss  (2.0792126549188463)	region_loss (3.3044995020959647)	overall_loss (3.0594421769316327)	Prec1  (70.4062271118164) 	
Training Epoch: [3][4200/4495]	att_loss  (2.0792109575818705)	region_loss (3.301688204774173)	overall_loss (3.057192799846946)	Prec1  (70.42445373535156) 	
Training Epoch: [3][4300/4495]	att_loss  (2.079212031012994)	region_loss (3.296814603712414)	overall_loss (3.0532941334027517)	Prec1  (70.47815704345703) 	
Training Epoch: [3][4400/4495]	att_loss  (2.0792129276503597)	region_loss (3.292198371464652)	overall_loss (3.0496013269617297)	Prec1  (70.50989532470703) 	
Testing [62/63]	att_loss  (2.079222843170166)	region_loss (5.833033763885498)	overall_loss (5.082271671295166)	Prec@1  (58.47500228881836)	
Epoch: 3   Test Acc: 58.47500228881836
Training Epoch: [4][0/4495]	att_loss  (2.0786893367767334)	region_loss (3.465449333190918)	overall_loss (3.1880974769592285)	Prec1  (70.3125) 	
Training Epoch: [4][100/4495]	att_loss  (2.079218649628139)	region_loss (3.0998230476190547)	overall_loss (2.8957022062622673)	Prec1  (71.81311798095703) 	
Training Epoch: [4][200/4495]	att_loss  (2.079229891003661)	region_loss (3.137019242220257)	overall_loss (2.9254614132553782)	Prec1  (71.46299743652344) 	
Training Epoch: [4][300/4495]	att_loss  (2.079221037139132)	region_loss (3.165182181371011)	overall_loss (2.947989991336962)	Prec1  (71.5687255859375) 	
Training Epoch: [4][400/4495]	att_loss  (2.0792188222271544)	region_loss (3.1555077878019757)	overall_loss (2.940250039397927)	Prec1  (71.62952423095703) 	
Training Epoch: [4][500/4495]	att_loss  (2.0792119074724393)	region_loss (3.1400498680011957)	overall_loss (2.927882320152785)	Prec1  (71.8001480102539) 	
Training Epoch: [4][600/4495]	att_loss  (2.0791974468358148)	region_loss (3.140628811325289)	overall_loss (2.9283425798035303)	Prec1  (71.77880096435547) 	
Training Epoch: [4][700/4495]	att_loss  (2.079198633213016)	region_loss (3.132751869407088)	overall_loss (2.9220412656686103)	Prec1  (71.82596588134766) 	
Training Epoch: [4][800/4495]	att_loss  (2.079198062196653)	region_loss (3.1267274362168807)	overall_loss (2.9172216025779907)	Prec1  (71.88670349121094) 	
Training Epoch: [4][900/4495]	att_loss  (2.0791930964466734)	region_loss (3.119835804755098)	overall_loss (2.911707304029433)	Prec1  (71.9183578491211) 	
Training Epoch: [4][1000/4495]	att_loss  (2.0791957218806583)	region_loss (3.1135844707965377)	overall_loss (2.90670676283784)	Prec1  (71.99519348144531) 	
Training Epoch: [4][1100/4495]	att_loss  (2.0792006283430053)	region_loss (3.113292440948001)	overall_loss (2.9064741201773217)	Prec1  (71.9970474243164) 	
Training Epoch: [4][1200/4495]	att_loss  (2.079203621533193)	region_loss (3.1220054592717004)	overall_loss (2.9134451332536964)	Prec1  (71.93875122070312) 	
Training Epoch: [4][1300/4495]	att_loss  (2.079205202744797)	region_loss (3.1123677310716364)	overall_loss (2.9057352669325174)	Prec1  (72.06595611572266) 	
Training Epoch: [4][1400/4495]	att_loss  (2.079197733169109)	region_loss (3.1070546939660617)	overall_loss (2.9014833436362832)	Prec1  (72.14935302734375) 	
Training Epoch: [4][1500/4495]	att_loss  (2.079196450076526)	region_loss (3.1025750342406564)	overall_loss (2.8978993587697213)	Prec1  (72.20394897460938) 	
Training Epoch: [4][1600/4495]	att_loss  (2.0791969302294775)	region_loss (3.0923725369570776)	overall_loss (2.8897374569066683)	Prec1  (72.3044204711914) 	
Training Epoch: [4][1700/4495]	att_loss  (2.079197934485407)	region_loss (3.086467278697503)	overall_loss (2.8850134510912664)	Prec1  (72.37470245361328) 	
Training Epoch: [4][1800/4495]	att_loss  (2.0791964405182664)	region_loss (3.0843144241536344)	overall_loss (2.8832908688751213)	Prec1  (72.41549682617188) 	
Training Epoch: [4][1900/4495]	att_loss  (2.079203633369113)	region_loss (3.0824182638050943)	overall_loss (2.8817753791181997)	Prec1  (72.43802642822266) 	
Training Epoch: [4][2000/4495]	att_loss  (2.0792076933211177)	region_loss (3.0794506332744427)	overall_loss (2.879402086831283)	Prec1  (72.4590835571289) 	
Training Epoch: [4][2100/4495]	att_loss  (2.0791998642617777)	region_loss (3.073544983521125)	overall_loss (2.8746760007825594)	Prec1  (72.50714111328125) 	
Training Epoch: [4][2200/4495]	att_loss  (2.0792004393751324)	region_loss (3.0655990184192925)	overall_loss (2.868319344108942)	Prec1  (72.57425689697266) 	
Training Epoch: [4][2300/4495]	att_loss  (2.0791984092458753)	region_loss (3.0603311574236303)	overall_loss (2.86410464923416)	Prec1  (72.6375732421875) 	
Training Epoch: [4][2400/4495]	att_loss  (2.07919897729285)	region_loss (3.05157974068198)	overall_loss (2.8571036298887673)	Prec1  (72.69432067871094) 	
Training Epoch: [4][2500/4495]	att_loss  (2.0791973525264273)	region_loss (3.048098720059019)	overall_loss (2.8543184883639316)	Prec1  (72.73152923583984) 	
Training Epoch: [4][2600/4495]	att_loss  (2.0791967828106026)	region_loss (3.0440548904031024)	overall_loss (2.851083311105132)	Prec1  (72.78750610351562) 	
Training Epoch: [4][2700/4495]	att_loss  (2.0791977762690474)	region_loss (3.037891115801549)	overall_loss (2.84615249008834)	Prec1  (72.85496520996094) 	
Training Epoch: [4][2800/4495]	att_loss  (2.0791977724404216)	region_loss (3.032574476407537)	overall_loss (2.8418991779778864)	Prec1  (72.90086364746094) 	
Training Epoch: [4][2900/4495]	att_loss  (2.079196062140611)	region_loss (3.026373182564347)	overall_loss (2.8369378007637636)	Prec1  (72.98614501953125) 	
Training Epoch: [4][3000/4495]	att_loss  (2.0791981302869593)	region_loss (3.023969157105166)	overall_loss (2.83501499416589)	Prec1  (73.01785278320312) 	
Training Epoch: [4][3100/4495]	att_loss  (2.079200110793767)	region_loss (3.019364002057715)	overall_loss (2.831331266198993)	Prec1  (73.06060028076172) 	
Training Epoch: [4][3200/4495]	att_loss  (2.0792015562650374)	region_loss (3.0126133076811685)	overall_loss (2.825930999540158)	Prec1  (73.10264587402344) 	
Training Epoch: [4][3300/4495]	att_loss  (2.079203553215658)	region_loss (3.010237876157694)	overall_loss (2.8240310539443794)	Prec1  (73.13219451904297) 	
Training Epoch: [4][3400/4495]	att_loss  (2.0792019277486826)	region_loss (3.0041889243180595)	overall_loss (2.8191915678648485)	Prec1  (73.18849182128906) 	
Training Epoch: [4][3500/4495]	att_loss  (2.07920191601936)	region_loss (2.9996681048236074)	overall_loss (2.815574909952224)	Prec1  (73.21702575683594) 	
Training Epoch: [4][3600/4495]	att_loss  (2.0792045620407404)	region_loss (2.999310866192757)	overall_loss (2.8152896481664667)	Prec1  (73.21273040771484) 	
Training Epoch: [4][3700/4495]	att_loss  (2.079203919506305)	region_loss (2.995816692504584)	overall_loss (2.812494181195197)	Prec1  (73.25680541992188) 	
Training Epoch: [4][3800/4495]	att_loss  (2.0792011505238355)	region_loss (2.993734278104331)	overall_loss (2.810827695580101)	Prec1  (73.27513122558594) 	
Training Epoch: [4][3900/4495]	att_loss  (2.0791980718472347)	region_loss (2.9907665120366596)	overall_loss (2.8084528670497995)	Prec1  (73.31413269042969) 	
Training Epoch: [4][4000/4495]	att_loss  (2.0791989895916676)	region_loss (2.9885875412715253)	overall_loss (2.8067098738580487)	Prec1  (73.3332290649414) 	
Training Epoch: [4][4100/4495]	att_loss  (2.0791949756666965)	region_loss (2.9853014113757124)	overall_loss (2.8040801672725033)	Prec1  (73.34872436523438) 	
Training Epoch: [4][4200/4495]	att_loss  (2.0791956095206285)	region_loss (2.9815197129443667)	overall_loss (2.8010549350909693)	Prec1  (73.40067291259766) 	
Training Epoch: [4][4300/4495]	att_loss  (2.0791981817816225)	region_loss (2.978636493740512)	overall_loss (2.7987488740212805)	Prec1  (73.43168640136719) 	
Training Epoch: [4][4400/4495]	att_loss  (2.079198379999831)	region_loss (2.9747788458080677)	overall_loss (2.7956627957307867)	Prec1  (73.45951080322266) 	
Testing [62/63]	att_loss  (2.0792629318237306)	region_loss (5.935492744445801)	overall_loss (5.164246883392334)	Prec@1  (57.775001525878906)	
Epoch: 4   Test Acc: 57.775001525878906
Training Epoch: [5][0/4495]	att_loss  (2.0795111656188965)	region_loss (2.382185935974121)	overall_loss (2.321650981903076)	Prec1  (82.8125) 	
Training Epoch: [5][100/4495]	att_loss  (2.07911214734068)	region_loss (2.9012266350264597)	overall_loss (2.7368037653441477)	Prec1  (73.70049285888672) 	
Training Epoch: [5][200/4495]	att_loss  (2.0791563691191413)	region_loss (2.8552454579528885)	overall_loss (2.700027681701812)	Prec1  (74.54912567138672) 	
Training Epoch: [5][300/4495]	att_loss  (2.079181439852794)	region_loss (2.8277008933482377)	overall_loss (2.6779970405901787)	Prec1  (74.90656280517578) 	
Training Epoch: [5][400/4495]	att_loss  (2.079165728014901)	region_loss (2.8366743204302325)	overall_loss (2.6851726452311375)	Prec1  (74.7662124633789) 	
Training Epoch: [5][500/4495]	att_loss  (2.079158139086055)	region_loss (2.8242294169709594)	overall_loss (2.675215203842955)	Prec1  (74.92826843261719) 	
Training Epoch: [5][600/4495]	att_loss  (2.079138097667853)	region_loss (2.8373702883522047)	overall_loss (2.685723895836194)	Prec1  (74.78160858154297) 	
Training Epoch: [5][700/4495]	att_loss  (2.0791193476417096)	region_loss (2.829228582973997)	overall_loss (2.6792067818226726)	Prec1  (74.96434020996094) 	
Training Epoch: [5][800/4495]	att_loss  (2.079117776749286)	region_loss (2.836216414912363)	overall_loss (2.6847967335049727)	Prec1  (74.92391967773438) 	
Training Epoch: [5][900/4495]	att_loss  (2.0791343226946153)	region_loss (2.835628685358494)	overall_loss (2.684329859689126)	Prec1  (74.9341049194336) 	
Training Epoch: [5][1000/4495]	att_loss  (2.0791309949282284)	region_loss (2.8323874144882826)	overall_loss (2.681736179045983)	Prec1  (74.92351531982422) 	
Training Epoch: [5][1100/4495]	att_loss  (2.0791442420242268)	region_loss (2.837631345120048)	overall_loss (2.6859339713833745)	Prec1  (74.8453140258789) 	
Training Epoch: [5][1200/4495]	att_loss  (2.079163867766216)	region_loss (2.8323230735467533)	overall_loss (2.6816912794192564)	Prec1  (74.88681030273438) 	
Training Epoch: [5][1300/4495]	att_loss  (2.079171650781712)	region_loss (2.8369450374899414)	overall_loss (2.6853904085467173)	Prec1  (74.85227966308594) 	
Training Epoch: [5][1400/4495]	att_loss  (2.0791732267683356)	region_loss (2.8406898332952517)	overall_loss (2.688386559826744)	Prec1  (74.79367065429688) 	
Training Epoch: [5][1500/4495]	att_loss  (2.0791893996531607)	region_loss (2.8372729384525868)	overall_loss (2.685656277756942)	Prec1  (74.85426330566406) 	
Training Epoch: [5][1600/4495]	att_loss  (2.07918854477553)	region_loss (2.829167906229828)	overall_loss (2.6791720797612024)	Prec1  (74.9638900756836) 	
Training Epoch: [5][1700/4495]	att_loss  (2.0791875709161416)	region_loss (2.823381429066173)	overall_loss (2.6745427024147217)	Prec1  (75.00550842285156) 	
Training Epoch: [5][1800/4495]	att_loss  (2.079179395906532)	region_loss (2.8173970117097693)	overall_loss (2.6697535337440708)	Prec1  (75.04164123535156) 	
Training Epoch: [5][1900/4495]	att_loss  (2.0791879198163636)	region_loss (2.8132692469476965)	overall_loss (2.666453026834756)	Prec1  (75.08301544189453) 	
Training Epoch: [5][2000/4495]	att_loss  (2.07918507393928)	region_loss (2.8055016815156)	overall_loss (2.6602384048006287)	Prec1  (75.15851593017578) 	
Training Epoch: [5][2100/4495]	att_loss  (2.0791820385863473)	region_loss (2.8039754290968846)	overall_loss (2.6590167953535695)	Prec1  (75.14725494384766) 	
Training Epoch: [5][2200/4495]	att_loss  (2.0791799348357154)	region_loss (2.8031595779624325)	overall_loss (2.658363693738623)	Prec1  (75.14340209960938) 	
Training Epoch: [5][2300/4495]	att_loss  (2.0791790456369825)	region_loss (2.799774719859558)	overall_loss (2.6556556292690954)	Prec1  (75.1751937866211) 	
Training Epoch: [5][2400/4495]	att_loss  (2.0791735902323123)	region_loss (2.7972338409634343)	overall_loss (2.653621834499148)	Prec1  (75.19588470458984) 	
Training Epoch: [5][2500/4495]	att_loss  (2.0791694312417857)	region_loss (2.7923683376133037)	overall_loss (2.6497285997138316)	Prec1  (75.22740936279297) 	
Training Epoch: [5][2600/4495]	att_loss  (2.0791661239779486)	region_loss (2.789226599538936)	overall_loss (2.6472145475730766)	Prec1  (75.27753448486328) 	
Training Epoch: [5][2700/4495]	att_loss  (2.079163790984932)	region_loss (2.7903932905868)	overall_loss (2.648147433565882)	Prec1  (75.26842498779297) 	
Training Epoch: [5][2800/4495]	att_loss  (2.0791606426409253)	region_loss (2.7842255470711006)	overall_loss (2.643212608761636)	Prec1  (75.32744598388672) 	
Training Epoch: [5][2900/4495]	att_loss  (2.0791656059546537)	region_loss (2.7824683606891703)	overall_loss (2.6418078524702295)	Prec1  (75.34308624267578) 	
Training Epoch: [5][3000/4495]	att_loss  (2.0791686424927804)	region_loss (2.7794698265066784)	overall_loss (2.6394096326128875)	Prec1  (75.357177734375) 	
Training Epoch: [5][3100/4495]	att_loss  (2.0791679717232743)	region_loss (2.776559608284791)	overall_loss (2.63708132368947)	Prec1  (75.36580657958984) 	
Training Epoch: [5][3200/4495]	att_loss  (2.0791689326188894)	region_loss (2.772430503230585)	overall_loss (2.6337782315483915)	Prec1  (75.40758514404297) 	
Training Epoch: [5][3300/4495]	att_loss  (2.0791735862898344)	region_loss (2.7695448143992847)	overall_loss (2.6314706114052786)	Prec1  (75.42364501953125) 	
Training Epoch: [5][3400/4495]	att_loss  (2.0791760464690427)	region_loss (2.769488753798568)	overall_loss (2.631426255585901)	Prec1  (75.4240493774414) 	
Training Epoch: [5][3500/4495]	att_loss  (2.0791781286415594)	region_loss (2.76878951726999)	overall_loss (2.6308672827538406)	Prec1  (75.43603515625) 	
Training Epoch: [5][3600/4495]	att_loss  (2.0791773367709365)	region_loss (2.7676112983532)	overall_loss (2.629924549125559)	Prec1  (75.4469223022461) 	
Training Epoch: [5][3700/4495]	att_loss  (2.0791760595770534)	region_loss (2.764918694880098)	overall_loss (2.627770210529978)	Prec1  (75.47369384765625) 	
Training Epoch: [5][3800/4495]	att_loss  (2.0791748194153827)	region_loss (2.7626766396083196)	overall_loss (2.6259763181538873)	Prec1  (75.48836517333984) 	
Training Epoch: [5][3900/4495]	att_loss  (2.0791764014869187)	region_loss (2.760814966528393)	overall_loss (2.6244872957888212)	Prec1  (75.50147247314453) 	
Training Epoch: [5][4000/4495]	att_loss  (2.079176177504181)	region_loss (2.758057904046823)	overall_loss (2.6222816009099588)	Prec1  (75.53229522705078) 	
Training Epoch: [5][4100/4495]	att_loss  (2.079173870768032)	region_loss (2.754592114644817)	overall_loss (2.61950850791973)	Prec1  (75.56616973876953) 	
Training Epoch: [5][4200/4495]	att_loss  (2.079173548849842)	region_loss (2.7519049634254706)	overall_loss (2.617358722649311)	Prec1  (75.59025573730469) 	
Training Epoch: [5][4300/4495]	att_loss  (2.0791736687374405)	region_loss (2.748602440262417)	overall_loss (2.614716727964769)	Prec1  (75.6194076538086) 	
Training Epoch: [5][4400/4495]	att_loss  (2.079172998741252)	region_loss (2.7460657158810236)	overall_loss (2.612687214237266)	Prec1  (75.6482925415039) 	
Testing [62/63]	att_loss  (2.0791966686248777)	region_loss (6.443823081970215)	overall_loss (5.570897857666016)	Prec@1  (57.60000228881836)	
Epoch: 5   Test Acc: 57.60000228881836
Training Epoch: [6][0/4495]	att_loss  (2.080124616622925)	region_loss (2.4737088680267334)	overall_loss (2.394991874694824)	Prec1  (81.25) 	
Training Epoch: [6][100/4495]	att_loss  (2.079329273488262)	region_loss (2.5991020426891818)	overall_loss (2.4951475162317256)	Prec1  (77.39789581298828) 	
Training Epoch: [6][200/4495]	att_loss  (2.0792391525572214)	region_loss (2.54795648090875)	overall_loss (2.454213052839782)	Prec1  (77.82959747314453) 	
Training Epoch: [6][300/4495]	att_loss  (2.0792637853527385)	region_loss (2.5668491940957763)	overall_loss (2.4693321484663953)	Prec1  (77.5695571899414) 	
Training Epoch: [6][400/4495]	att_loss  (2.0792525195124143)	region_loss (2.5903013448168215)	overall_loss (2.4880916195320073)	Prec1  (77.345703125) 	
Training Epoch: [6][500/4495]	att_loss  (2.0792322196884307)	region_loss (2.577886002268382)	overall_loss (2.4781552829190403)	Prec1  (77.40144348144531) 	
Training Epoch: [6][600/4495]	att_loss  (2.0792204488732056)	region_loss (2.580951890413852)	overall_loss (2.4806056379676855)	Prec1  (77.39704132080078) 	
Training Epoch: [6][700/4495]	att_loss  (2.0792128604420923)	region_loss (2.5821783398085416)	overall_loss (2.481585280531313)	Prec1  (77.34709930419922) 	
Training Epoch: [6][800/4495]	att_loss  (2.0792038860392483)	region_loss (2.5822258256050232)	overall_loss (2.481621476892526)	Prec1  (77.33887481689453) 	
Training Epoch: [6][900/4495]	att_loss  (2.079200188142478)	region_loss (2.5793687339363562)	overall_loss (2.479335063570215)	Prec1  (77.36543273925781) 	
Training Epoch: [6][1000/4495]	att_loss  (2.079198233016602)	region_loss (2.5753847262956997)	overall_loss (2.476147466963464)	Prec1  (77.37886810302734) 	
Training Epoch: [6][1100/4495]	att_loss  (2.0791932139799445)	region_loss (2.5771787654909626)	overall_loss (2.477581694816915)	Prec1  (77.39271545410156) 	
Training Epoch: [6][1200/4495]	att_loss  (2.079192909968088)	region_loss (2.58122346691049)	overall_loss (2.4808173941930662)	Prec1  (77.30276489257812) 	
Training Epoch: [6][1300/4495]	att_loss  (2.0791895424375895)	region_loss (2.5782301430515284)	overall_loss (2.4784220611013694)	Prec1  (77.3287353515625) 	
Training Epoch: [6][1400/4495]	att_loss  (2.0791862868650055)	region_loss (2.5720629715902477)	overall_loss (2.473487671999145)	Prec1  (77.39002990722656) 	
Training Epoch: [6][1500/4495]	att_loss  (2.0791857677805354)	region_loss (2.5691835665051577)	overall_loss (2.4711840448182554)	Prec1  (77.39111328125) 	
Training Epoch: [6][1600/4495]	att_loss  (2.0791850302980364)	region_loss (2.5664351285806974)	overall_loss (2.468985146392069)	Prec1  (77.39596557617188) 	
Training Epoch: [6][1700/4495]	att_loss  (2.0791754836127874)	region_loss (2.564880523942345)	overall_loss (2.4677395535384394)	Prec1  (77.41217803955078) 	
Training Epoch: [6][1800/4495]	att_loss  (2.0791758327335863)	region_loss (2.5723731839478114)	overall_loss (2.4737337513939)	Prec1  (77.34504699707031) 	
Training Epoch: [6][1900/4495]	att_loss  (2.0791768483398214)	region_loss (2.5700165076609474)	overall_loss (2.4718486136477096)	Prec1  (77.39430236816406) 	
Training Epoch: [6][2000/4495]	att_loss  (2.079176576777377)	region_loss (2.5686352471599934)	overall_loss (2.470743551068399)	Prec1  (77.41363525390625) 	
Training Epoch: [6][2100/4495]	att_loss  (2.0791770978407427)	region_loss (2.569075457996439)	overall_loss (2.471095823833115)	Prec1  (77.38428497314453) 	
Training Epoch: [6][2200/4495]	att_loss  (2.0791718208047816)	region_loss (2.565208459377072)	overall_loss (2.4680011693806283)	Prec1  (77.41793823242188) 	
Training Epoch: [6][2300/4495]	att_loss  (2.079171147879079)	region_loss (2.5645193653695224)	overall_loss (2.4674497592557985)	Prec1  (77.39705657958984) 	
Training Epoch: [6][2400/4495]	att_loss  (2.079172002132214)	region_loss (2.563896867991586)	overall_loss (2.4669519330203062)	Prec1  (77.37140655517578) 	
Training Epoch: [6][2500/4495]	att_loss  (2.079175449189831)	region_loss (2.5623032642049535)	overall_loss (2.4656777391429903)	Prec1  (77.38154602050781) 	
Training Epoch: [6][2600/4495]	att_loss  (2.0791739971992467)	region_loss (2.5601422447555113)	overall_loss (2.4639486332024028)	Prec1  (77.39811706542969) 	
Training Epoch: [6][2700/4495]	att_loss  (2.0791778193717088)	region_loss (2.559382077147898)	overall_loss (2.463341263213541)	Prec1  (77.39089965820312) 	
Training Epoch: [6][2800/4495]	att_loss  (2.0791772854664377)	region_loss (2.556324349391124)	overall_loss (2.460894973564897)	Prec1  (77.42770385742188) 	
Training Epoch: [6][2900/4495]	att_loss  (2.0791788428292444)	region_loss (2.554785655351065)	overall_loss (2.4596643296655643)	Prec1  (77.45819854736328) 	
Training Epoch: [6][3000/4495]	att_loss  (2.0791801951877757)	region_loss (2.55155379360654)	overall_loss (2.457079110364841)	Prec1  (77.48355102539062) 	
Training Epoch: [6][3100/4495]	att_loss  (2.079181669528005)	region_loss (2.5489484569942595)	overall_loss (2.4549951357673883)	Prec1  (77.50524139404297) 	
Training Epoch: [6][3200/4495]	att_loss  (2.0791833006713736)	region_loss (2.5478091736355264)	overall_loss (2.4540840359041)	Prec1  (77.52997589111328) 	
Training Epoch: [6][3300/4495]	att_loss  (2.079182477258834)	region_loss (2.5443842233003755)	overall_loss (2.4513439110863104)	Prec1  (77.54894256591797) 	
Training Epoch: [6][3400/4495]	att_loss  (2.079181473383445)	region_loss (2.5401984889238802)	overall_loss (2.4479951232505246)	Prec1  (77.5750732421875) 	
Training Epoch: [6][3500/4495]	att_loss  (2.079182640820427)	region_loss (2.538928573896326)	overall_loss (2.4469794247770813)	Prec1  (77.59122467041016) 	
Training Epoch: [6][3600/4495]	att_loss  (2.079179950437093)	region_loss (2.53861523826597)	overall_loss (2.4467282185982215)	Prec1  (77.59042358398438) 	
Training Epoch: [6][3700/4495]	att_loss  (2.0791792019867503)	region_loss (2.5383182643265765)	overall_loss (2.4464904898664366)	Prec1  (77.5879898071289) 	
Training Epoch: [6][3800/4495]	att_loss  (2.079179749241693)	region_loss (2.5353978215252466)	overall_loss (2.4441542448542113)	Prec1  (77.62801361083984) 	
Training Epoch: [6][3900/4495]	att_loss  (2.0791790408985458)	region_loss (2.534854143743361)	overall_loss (2.443719160767648)	Prec1  (77.61991882324219) 	
Training Epoch: [6][4000/4495]	att_loss  (2.079178870007325)	region_loss (2.532332991814798)	overall_loss (2.4417022046849777)	Prec1  (77.63996887207031) 	
Training Epoch: [6][4100/4495]	att_loss  (2.0791777290678177)	region_loss (2.530029730400322)	overall_loss (2.4398593670913167)	Prec1  (77.67617797851562) 	
Training Epoch: [6][4200/4495]	att_loss  (2.079176797778287)	region_loss (2.527031671288637)	overall_loss (2.437460733407794)	Prec1  (77.69429779052734) 	
Training Epoch: [6][4300/4495]	att_loss  (2.0791782295667858)	region_loss (2.5258613293793446)	overall_loss (2.436524746085954)	Prec1  (77.71194458007812) 	
Training Epoch: [6][4400/4495]	att_loss  (2.0791795053310866)	region_loss (2.523437853916968)	overall_loss (2.434586220777893)	Prec1  (77.73943328857422) 	
Testing [62/63]	att_loss  (2.0792716331481933)	region_loss (7.137818298339844)	overall_loss (6.126109062194824)	Prec@1  (56.650001525878906)	
Epoch: 6   Test Acc: 56.650001525878906
Training Epoch: [7][0/4495]	att_loss  (2.0808448791503906)	region_loss (3.09734845161438)	overall_loss (2.894047737121582)	Prec1  (70.3125) 	
Training Epoch: [7][100/4495]	att_loss  (2.0791571211106707)	region_loss (2.41550338740396)	overall_loss (2.3482341612919724)	Prec1  (78.8675765991211) 	
Training Epoch: [7][200/4495]	att_loss  (2.0790969044414918)	region_loss (2.420250387927193)	overall_loss (2.3520197227819644)	Prec1  (79.10447692871094) 	
Training Epoch: [7][300/4495]	att_loss  (2.0791214383717787)	region_loss (2.410583100841687)	overall_loss (2.34429079987282)	Prec1  (78.87770080566406) 	
Training Epoch: [7][400/4495]	att_loss  (2.079130461685675)	region_loss (2.417452464079916)	overall_loss (2.349788097669359)	Prec1  (78.87702941894531) 	
Training Epoch: [7][500/4495]	att_loss  (2.079139483427097)	region_loss (2.429582591304284)	overall_loss (2.3594940068003183)	Prec1  (78.81736755371094) 	
Training Epoch: [7][600/4495]	att_loss  (2.0791314044927005)	region_loss (2.43059112863017)	overall_loss (2.3602992206960667)	Prec1  (78.78275299072266) 	
Training Epoch: [7][700/4495]	att_loss  (2.07911807698292)	region_loss (2.410182761057637)	overall_loss (2.3439698605326544)	Prec1  (78.96086120605469) 	
Training Epoch: [7][800/4495]	att_loss  (2.079133688287342)	region_loss (2.408582255783748)	overall_loss (2.3426925784788475)	Prec1  (79.04767608642578) 	
Training Epoch: [7][900/4495]	att_loss  (2.079132287272073)	region_loss (2.4080986137792353)	overall_loss (2.3423053853122826)	Prec1  (79.0250473022461) 	
Training Epoch: [7][1000/4495]	att_loss  (2.079138137005664)	region_loss (2.412569296348107)	overall_loss (2.3458831017310326)	Prec1  (78.99288177490234) 	
Training Epoch: [7][1100/4495]	att_loss  (2.0791411124826236)	region_loss (2.413378396974056)	overall_loss (2.346530977755)	Prec1  (78.94102478027344) 	
Training Epoch: [7][1200/4495]	att_loss  (2.079137051730827)	region_loss (2.4088030999943575)	overall_loss (2.3428699280399763)	Prec1  (78.9537353515625) 	
Training Epoch: [7][1300/4495]	att_loss  (2.0791285879148327)	region_loss (2.4046053137804893)	overall_loss (2.33951000558881)	Prec1  (78.9656982421875) 	
Training Epoch: [7][1400/4495]	att_loss  (2.0791223563099654)	region_loss (2.403391348558354)	overall_loss (2.3385375863224334)	Prec1  (78.94026184082031) 	
Training Epoch: [7][1500/4495]	att_loss  (2.0791210451577204)	region_loss (2.40157394278931)	overall_loss (2.337083398858997)	Prec1  (78.95777893066406) 	
Training Epoch: [7][1600/4495]	att_loss  (2.0791170403482315)	region_loss (2.4025997233941805)	overall_loss (2.3379032224658727)	Prec1  (78.93797302246094) 	
Training Epoch: [7][1700/4495]	att_loss  (2.0791189280907454)	region_loss (2.4014918033267945)	overall_loss (2.337017264315691)	Prec1  (78.93701934814453) 	
Training Epoch: [7][1800/4495]	att_loss  (2.079126546940229)	region_loss (2.399257846172487)	overall_loss (2.3352316220822034)	Prec1  (78.950927734375) 	
Training Epoch: [7][1900/4495]	att_loss  (2.07912955868564)	region_loss (2.4008605395537312)	overall_loss (2.3365143786475007)	Prec1  (78.97981262207031) 	
Training Epoch: [7][2000/4495]	att_loss  (2.07912991477036)	region_loss (2.397842822225019)	overall_loss (2.3341002760977223)	Prec1  (79.03002166748047) 	
Training Epoch: [7][2100/4495]	att_loss  (2.079128918790749)	region_loss (2.3994698119356426)	overall_loss (2.335401669052429)	Prec1  (79.02487182617188) 	
Training Epoch: [7][2200/4495]	att_loss  (2.0791259317818365)	region_loss (2.3956625424748603)	overall_loss (2.332355256461924)	Prec1  (79.06136322021484) 	
Training Epoch: [7][2300/4495]	att_loss  (2.079122506344334)	region_loss (2.395907683043001)	overall_loss (2.332550683523043)	Prec1  (79.0369644165039) 	
Training Epoch: [7][2400/4495]	att_loss  (2.0791164082023115)	region_loss (2.3940920289086085)	overall_loss (2.3310969410117393)	Prec1  (79.0582046508789) 	
Training Epoch: [7][2500/4495]	att_loss  (2.079119388697768)	region_loss (2.3908420060930706)	overall_loss (2.328497518734282)	Prec1  (79.0846176147461) 	
Training Epoch: [7][2600/4495]	att_loss  (2.079122473028521)	region_loss (2.388359216486936)	overall_loss (2.3265119041676066)	Prec1  (79.1083984375) 	
Training Epoch: [7][2700/4495]	att_loss  (2.0791293415746086)	region_loss (2.3880950253524413)	overall_loss (2.326301924699503)	Prec1  (79.10901641845703) 	
Training Epoch: [7][2800/4495]	att_loss  (2.0791296781024435)	region_loss (2.386195516382018)	overall_loss (2.324782384842134)	Prec1  (79.11404418945312) 	
Training Epoch: [7][2900/4495]	att_loss  (2.079128617789983)	region_loss (2.3849319184249698)	overall_loss (2.323771294171709)	Prec1  (79.12249755859375) 	
Training Epoch: [7][3000/4495]	att_loss  (2.0791313322493727)	region_loss (2.383830756753415)	overall_loss (2.322890907555809)	Prec1  (79.14289093017578) 	
Training Epoch: [7][3100/4495]	att_loss  (2.079128120991923)	region_loss (2.380793831033962)	overall_loss (2.320460724738366)	Prec1  (79.1856460571289) 	
Training Epoch: [7][3200/4495]	att_loss  (2.079131826912005)	region_loss (2.3788927544358742)	overall_loss (2.318940604600784)	Prec1  (79.19790649414062) 	
Training Epoch: [7][3300/4495]	att_loss  (2.079132131647609)	region_loss (2.375452671149253)	overall_loss (2.3161885988636475)	Prec1  (79.2203140258789) 	
Training Epoch: [7][3400/4495]	att_loss  (2.079133412095597)	region_loss (2.3762085753235875)	overall_loss (2.3167935784723226)	Prec1  (79.20602416992188) 	
Training Epoch: [7][3500/4495]	att_loss  (2.0791353482717922)	region_loss (2.3755553508955765)	overall_loss (2.316271386021241)	Prec1  (79.22334289550781) 	
Training Epoch: [7][3600/4495]	att_loss  (2.079134613763819)	region_loss (2.3746004218162677)	overall_loss (2.315507295627589)	Prec1  (79.2305908203125) 	
Training Epoch: [7][3700/4495]	att_loss  (2.0791349504419805)	region_loss (2.3727013798411942)	overall_loss (2.313988129244209)	Prec1  (79.25898742675781) 	
Training Epoch: [7][3800/4495]	att_loss  (2.07913354552755)	region_loss (2.371631218959896)	overall_loss (2.31313171944346)	Prec1  (79.24806213378906) 	
Training Epoch: [7][3900/4495]	att_loss  (2.0791317287147795)	region_loss (2.370098500305553)	overall_loss (2.311905180604236)	Prec1  (79.23649597167969) 	
Training Epoch: [7][4000/4495]	att_loss  (2.0791320424174047)	region_loss (2.36719114146987)	overall_loss (2.3095793562273896)	Prec1  (79.25675201416016) 	
Training Epoch: [7][4100/4495]	att_loss  (2.079131582137347)	region_loss (2.365831507760936)	overall_loss (2.308491557204296)	Prec1  (79.27716064453125) 	
Training Epoch: [7][4200/4495]	att_loss  (2.0791330945347073)	region_loss (2.3630213494869508)	overall_loss (2.306243732837631)	Prec1  (79.29659271240234) 	
Training Epoch: [7][4300/4495]	att_loss  (2.0791335151351293)	region_loss (2.3612736715491054)	overall_loss (2.304845674346697)	Prec1  (79.31839752197266) 	
Training Epoch: [7][4400/4495]	att_loss  (2.079133800256526)	region_loss (2.3585126869680124)	overall_loss (2.302636943874346)	Prec1  (79.3502197265625) 	
Testing [62/63]	att_loss  (2.0791940307617187)	region_loss (7.387637504577636)	overall_loss (6.3259488906860355)	Prec@1  (57.82500457763672)	
Epoch: 7   Test Acc: 57.82500457763672
